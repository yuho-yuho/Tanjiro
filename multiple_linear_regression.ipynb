{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code for testing \"multiple_linear_regression\"\n",
    "\n",
    "### y = w1 * x1 + w2 * x2 + w3 * x3 + ... + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv('./mlr/Salary_Data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### label encoding: e.g., degree: 0, 1, 2 ...\n",
    "data[\"EducationLevel\"] = data[\"EducationLevel\"].map({\"高中以下\": 0, \"大學\": 1, \"碩士以上\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:395: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_array(X, dtype=np.int)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_int = np.zeros((n_samples, n_features), dtype=np.int)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:111: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mask = np.ones((n_samples, n_features), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "### one hot encoding\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder.fit(data[[\"City\"]])\n",
    "\n",
    "city_encoded = onehot_encoder.transform(data[[\"City\"]]).toarray()\n",
    "\n",
    "data[[\"CityA\", \"CityB\", \"CityC\"]] = city_encoded\n",
    "data = data.drop([\"City\", \"CityC\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data[[\"YearsExperience\", \"EducationLevel\", \"CityA\", \"CityB\"]]\n",
    "y = data[\"Salary\"]\n",
    "\n",
    "# separate training & testing sets: 20% used for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=86) \n",
    "# random_state means do not change the data selected, if change, use other numbers ... \n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "#print (len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary = w1 * YearsExperience + w2 * EducationLevel + w3 * CityA + w4 * CityB + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "w = np.array([1, 2, 3, 4])\n",
    "b = 1.0\n",
    "\n",
    "y_pred = (x_train*w).sum(axis = 1) + b # sum of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = ((y_train - y_pred)**2).mean()\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    This funct is to calculate cost function\n",
    "    \"\"\"\n",
    "    y_pred = (x*w).sum(axis = 1) + b\n",
    "    cost = ((y - y_pred)**2).mean()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1818.4746428571423"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(x_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w1 - w1_grad * learning_rate\n",
    "### w2 - w2_grad * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cost = (y - y_pred)**2 = (y - (w1*x1+w2*x2+w3*x3+w4*x4+b)**2\n",
    "y_pred = (x_train*w).sum(axis = 1) + b \n",
    "b_grad = (y_pred - y_train).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(x, y, w, b):\n",
    "    \"\"\"\n",
    "    This funct used to calculate gradients\n",
    "    \"\"\"\n",
    "    y_pred = (x*w).sum(axis=1) + b \n",
    "    ll = x.shape[1]; w_grad = np.zeros(ll)\n",
    "    b_grad = (y_pred - y).mean()\n",
    "    for i in range(ll):\n",
    "        w_grad[i] = (x[:,i]*(y_pred - y)).mean()\n",
    "    \n",
    "    return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.19e+02, -5.82e+01, -1.58e+01, -4.99e+00]), -39.78214285714285)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1, 2, 3, 4])\n",
    "b = 1.0\n",
    "\n",
    "compute_grad(x_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850.703214285714\n",
      "1744.3700355532972\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1, 2, 2, 4])\n",
    "b = 1.0\n",
    "learning_rate = 0.001\n",
    "w_grad, b_grad = compute_grad(x_train, y_train, w, b)\n",
    "\n",
    "# old cost\n",
    "print (compute_cost(x_train, y_train, w, b))\n",
    "    \n",
    "w = w - w_grad*learning_rate\n",
    "b = b - b_grad*learning_rate\n",
    "\n",
    "print (compute_cost(x_train, y_train, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: .2e}'.format})\n",
    "\n",
    "def grad_descent(x, y, w_init, b_init, l_rate, cost_funct, grad_funct, run_inter, p_inter = 1000):\n",
    "    \"\"\"\n",
    "    This funct used to calculate the gradient descent with input changes\n",
    "    \"\"\"\n",
    "    c_hist = []; w_hist = []; b_hist = []\n",
    "    \n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    \n",
    "    for i in range(run_inter):\n",
    "        w_gradient, b_gradient = compute_grad(x, y, w, b)\n",
    "\n",
    "        w = w - w_gradient * learning_rate\n",
    "        b = b - b_gradient * learning_rate\n",
    "        cost = compute_cost(x,y,w,b)\n",
    "        \n",
    "        c_hist.append(cost)\n",
    "        w_hist.append(w)\n",
    "        b_hist.append(b)\n",
    "        \n",
    "        if i%p_inter == 0:\n",
    "            print (f\"Ieration {i:5}: Cost {cost: .5e}, w: {w}, b: {b}\")\n",
    "    \n",
    "    return w, b, c_hist, w_hist, b_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ieration     0: Cost  1.82101e+03, w: [ 1.23e+00  2.06e+00  2.02e+00  4.01e+00], b: 0.041317857142857135\n",
      "Ieration  1000: Cost  1.32320e+02, w: [ 4.63e+00  1.26e+01  3.98e+00  2.44e+00], b: 5.703698438557687\n",
      "Ieration  2000: Cost  6.47313e+01, w: [ 3.27e+00  1.72e+01  4.80e+00  1.12e+00], b: 8.468380003492305\n",
      "Ieration  3000: Cost  4.53617e+01, w: [ 2.57e+00  1.93e+01  5.15e+00  5.60e-02], b: 10.166839940194103\n",
      "Ieration  4000: Cost  3.88260e+01, w: [ 2.22e+00  2.04e+01  5.20e+00 -8.39e-01], b: 11.281130634626818\n",
      "Ieration  5000: Cost  3.58606e+01, w: [ 2.05e+00  2.08e+01  5.08e+00 -1.62e+00], b: 12.069866805155327\n",
      "Ieration  6000: Cost  3.39947e+01, w: [ 1.96e+00  2.09e+01  4.87e+00 -2.32e+00], b: 12.672785725294537\n",
      "Ieration  7000: Cost  3.25480e+01, w: [ 1.92e+00  2.09e+01  4.60e+00 -2.95e+00], b: 13.16600374498556\n",
      "Ieration  8000: Cost  3.13210e+01, w: [ 1.90e+00  2.09e+01  4.30e+00 -3.54e+00], b: 13.59138114176089\n",
      "Ieration  9000: Cost  3.02462e+01, w: [ 1.90e+00  2.08e+01  4.00e+00 -4.08e+00], b: 13.97221737131694\n",
      "Ieration 10000: Cost  2.92946e+01, w: [ 1.90e+00  2.07e+01  3.70e+00 -4.58e+00], b: 14.321689287484785\n",
      "Ieration 11000: Cost  2.84486e+01, w: [ 1.91e+00  2.06e+01  3.41e+00 -5.05e+00], b: 14.6474146606253\n",
      "Ieration 12000: Cost  2.76956e+01, w: [ 1.91e+00  2.05e+01  3.12e+00 -5.49e+00], b: 14.953934865901713\n",
      "Ieration 13000: Cost  2.70247e+01, w: [ 1.92e+00  2.04e+01  2.85e+00 -5.90e+00], b: 15.244073562508444\n",
      "Ieration 14000: Cost  2.64268e+01, w: [ 1.93e+00  2.03e+01  2.59e+00 -6.28e+00], b: 15.519684674339844\n",
      "Ieration 15000: Cost  2.58938e+01, w: [ 1.93e+00  2.02e+01  2.35e+00 -6.64e+00], b: 15.782066657867501\n",
      "Ieration 16000: Cost  2.54183e+01, w: [ 1.94e+00  2.01e+01  2.11e+00 -6.98e+00], b: 16.03219338941606\n",
      "Ieration 17000: Cost  2.49942e+01, w: [ 1.94e+00  2.00e+01  1.89e+00 -7.29e+00], b: 16.270843730660506\n",
      "Ieration 18000: Cost  2.46157e+01, w: [ 1.94e+00  1.99e+01  1.68e+00 -7.58e+00], b: 16.498674813928137\n",
      "Ieration 19000: Cost  2.42778e+01, w: [ 1.95e+00  1.99e+01  1.48e+00 -7.86e+00], b: 16.716263903353802\n"
     ]
    }
   ],
   "source": [
    "### Now, use the function\n",
    "\n",
    "w_init = np.array([1, 2, 2, 4])\n",
    "b_init = 0\n",
    "l_rate = 2.0e-3\n",
    "run_inter = 20000\n",
    "w_final, b_final, c_hist, w_hist, b_hist = grad_descent(x_train, y_train, w_init, b_init, l_rate, compute_cost, compute_grad, run_inter, p_inter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.95e+00,  1.98e+01,  1.28e+00, -8.11e+00]), 16.923929443026303)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_final, b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28.357258</td>\n",
       "      <td>31.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46.393795</td>\n",
       "      <td>36.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70.480844</td>\n",
       "      <td>72.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>67.914612</td>\n",
       "      <td>63.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71.261555</td>\n",
       "      <td>70.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>52.721647</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45.613083</td>\n",
       "      <td>48.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.734073</td>\n",
       "      <td>80.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_pred  y_test\n",
       "19  28.357258    31.6\n",
       "35  46.393795    36.7\n",
       "16  70.480844    72.7\n",
       "22  67.914612    63.6\n",
       "7   71.261555    70.3\n",
       "24  52.721647    48.3\n",
       "11  45.613083    48.3\n",
       "1   71.734073    80.5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (w_final*x_test).sum(axis=1) + b_final\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"y_pred\": y_pred,\n",
    "    \"y_test\": y_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.070264027069683"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(x_test, y_test, w_final, b_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use Feature Scaling to increase the speed of gradient descent \n",
    "\n",
    "### x1 ~ (1, 10) while x2, x3 and x4 ~ (0, 2)\n",
    "\n",
    "### method: standardization = (x - mean(x)) / sdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.43e-02, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [ 1.25e+00, -2.22e-01, -1.07e+00, -3.46e-01],\n",
       "       [-1.03e+00, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [-1.81e-02, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [ 1.18e+00, -2.22e-01, -1.07e+00, -3.46e-01],\n",
       "       [-3.44e-01, -2.22e-01,  9.31e-01, -3.46e-01],\n",
       "       [-1.43e+00,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [-1.10e+00, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [ 1.03e+00, -2.22e-01, -1.07e+00, -3.46e-01],\n",
       "       [-7.78e-01, -2.22e-01,  9.31e-01, -3.46e-01],\n",
       "       [-1.99e-01, -2.22e-01,  9.31e-01, -3.46e-01],\n",
       "       [ 8.87e-01,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [ 1.76e+00,  1.02e+00, -1.07e+00,  2.89e+00],\n",
       "       [-1.32e+00,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [-5.61e-01, -2.22e-01,  9.31e-01, -3.46e-01],\n",
       "       [ 8.14e-01,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [-1.21e+00,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [ 1.76e+00,  1.02e+00, -1.07e+00,  2.89e+00],\n",
       "       [-1.27e-01, -2.22e-01,  9.31e-01, -3.46e-01],\n",
       "       [ 1.76e+00,  1.02e+00, -1.07e+00,  2.89e+00],\n",
       "       [ 4.16e-01,  1.02e+00,  9.31e-01, -3.46e-01],\n",
       "       [ 7.42e-01,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [ 3.44e-01,  1.02e+00,  9.31e-01, -3.46e-01],\n",
       "       [-9.59e-01, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [-9.95e-01, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [-5.43e-02, -1.47e+00,  9.31e-01, -3.46e-01],\n",
       "       [-1.25e+00,  1.02e+00, -1.07e+00, -3.46e-01],\n",
       "       [-5.97e-01, -2.22e-01,  9.31e-01, -3.46e-01]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) # only use training data, no testing data!!\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ieration     0: Cost  2.77242e+03, w: [ 1.01e+00  2.01e+00  1.99e+00  4.00e+00], b: 0.050325\n",
      "Ieration  1000: Cost  3.96064e+02, w: [ 2.71e+00  9.05e+00 -2.66e+00  2.05e+00], b: 31.839232020652823\n",
      "Ieration  2000: Cost  7.90508e+01, w: [ 3.04e+00  1.13e+01 -3.19e+00 -9.37e-02], b: 43.52786769061649\n",
      "Ieration  3000: Cost  3.27807e+01, w: [ 3.46e+00  1.24e+01 -3.01e+00 -1.37e+00], b: 47.82572554827677\n",
      "Ieration  4000: Cost  2.48038e+01, w: [ 3.89e+00  1.31e+01 -2.70e+00 -2.13e+00], b: 49.40602821885431\n",
      "Ieration  5000: Cost  2.28339e+01, w: [ 4.24e+00  1.35e+01 -2.40e+00 -2.59e+00], b: 49.987098280578984\n",
      "Ieration  6000: Cost  2.20953e+01, w: [ 4.52e+00  1.38e+01 -2.14e+00 -2.89e+00], b: 50.20075508374654\n",
      "Ieration  7000: Cost  2.17423e+01, w: [ 4.72e+00  1.40e+01 -1.94e+00 -3.09e+00], b: 50.27931571274255\n",
      "Ieration  8000: Cost  2.15580e+01, w: [ 4.87e+00  1.42e+01 -1.78e+00 -3.23e+00], b: 50.308202096591536\n",
      "Ieration  9000: Cost  2.14589e+01, w: [ 4.98e+00  1.43e+01 -1.65e+00 -3.33e+00], b: 50.31882348777092\n",
      "Ieration 10000: Cost  2.14052e+01, w: [ 5.05e+00  1.44e+01 -1.56e+00 -3.39e+00], b: 50.322728924712315\n",
      "Ieration 11000: Cost  2.13758e+01, w: [ 5.11e+00  1.44e+01 -1.49e+00 -3.44e+00], b: 50.32416493600741\n",
      "Ieration 12000: Cost  2.13598e+01, w: [ 5.15e+00  1.45e+01 -1.44e+00 -3.47e+00], b: 50.324692950790656\n",
      "Ieration 13000: Cost  2.13510e+01, w: [ 5.18e+00  1.45e+01 -1.40e+00 -3.50e+00], b: 50.32488709941056\n",
      "Ieration 14000: Cost  2.13461e+01, w: [ 5.20e+00  1.46e+01 -1.37e+00 -3.51e+00], b: 50.32495848696974\n",
      "Ieration 15000: Cost  2.13434e+01, w: [ 5.21e+00  1.46e+01 -1.34e+00 -3.53e+00], b: 50.32498473584869\n",
      "Ieration 16000: Cost  2.13420e+01, w: [ 5.23e+00  1.46e+01 -1.33e+00 -3.53e+00], b: 50.324994387441414\n",
      "Ieration 17000: Cost  2.13412e+01, w: [ 5.23e+00  1.46e+01 -1.31e+00 -3.54e+00], b: 50.324997936287865\n",
      "Ieration 18000: Cost  2.13407e+01, w: [ 5.24e+00  1.46e+01 -1.30e+00 -3.55e+00], b: 50.32499924118252\n",
      "Ieration 19000: Cost  2.13405e+01, w: [ 5.24e+00  1.46e+01 -1.30e+00 -3.55e+00], b: 50.32499972098632\n"
     ]
    }
   ],
   "source": [
    "w_final, b_final, c_hist, w_hist, b_hist = grad_descent(x_train, y_train, w_init, b_init, l_rate, compute_cost, compute_grad, run_inter, p_inter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
